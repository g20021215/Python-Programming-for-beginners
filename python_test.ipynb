{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "with open('train.txt', 'r') as f:\n",
    "    docs = f.read().splitlines() \n",
    "    f.close()\n",
    "with open('stop_words.txt', 'r') as g:\n",
    "    stop = g.read().splitlines() \n",
    "    g.close()    \n",
    "a = []\n",
    "positive = []\n",
    "negative = []\n",
    "neutral = []\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    b =docs[i].split(',')\n",
    "    for j in range(len(b)):\n",
    "        c = (b[1],b[2])\n",
    "        for k in c:\n",
    "            if k =='positive':\n",
    "                positive.append(c)\n",
    "                positive = list(set(positive))\n",
    "            if k =='negative':\n",
    "                negative.append(c)\n",
    "                negative = list(set(negative))\n",
    "            if k =='neutral':\n",
    "                neutral.append(c)\n",
    "                neutral = list(set(neutral))\n",
    "\n",
    "#句子初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一次输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "docs2 = []\n",
    "for (sentiment,words) in positive + negative :\n",
    "    words_filtered = [e.lower() for e in words.split() if len(e) >= 3]  \n",
    "    docs2.append((words_filtered,sentiment ))\n",
    "#切割句子\n",
    "test_docs2 = docs2\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_docs2(docs2):\n",
    "    all_words = []\n",
    "    for (sentiment,words) in docs2:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)  \n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "word_features = get_word_features(get_words_in_docs2(docs2))  \n",
    "' '.join(word_features)  \n",
    "def extract_features(document):\n",
    "    document_words = set(document)  \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)   \n",
    "    return features \n",
    "#提取特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = nltk.classify.util.apply_features(extract_features,docs2) \n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)  \n",
    "def train(labeled_featuresets, estimator=nltk.probability.ELEProbDist):\n",
    "    # Create the P(label) distribution\n",
    "    label_probdist = estimator(label_freqdist)\n",
    "    # Create the P(fval|label, fname) distribution\n",
    "    feature_probdist = {}\n",
    "    model = NaiveBayesClassifier(label_probdist, feature_probdist)\n",
    "    return model\n",
    "#朴素贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy: 70.151216% (1067/5).\n"
     ]
    }
   ],
   "source": [
    "def classify_docs(docs3):\n",
    "    return classifier.classify(extract_features(docs3)) \n",
    "total = accuracy = float(len(test_docs2))\n",
    "for docs3 in test_docs2:\n",
    "    if classify_docs(docs3[0]) != docs3[1]:\n",
    "        accuracy -= 1\n",
    "print('Total accuracy: %f%% (%d/5).' % (accuracy / total * 100, accuracy))\n",
    "#验证效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutPut1 = []\n",
    "with open('test_x.txt', 'r') as d:\n",
    "    docs_test = d.read().splitlines() \n",
    "    d.close()\n",
    "    for TEST in docs_test:\n",
    "        docs_test = TEST\n",
    "        OutPut1.append ( classifier.classify(extract_features(docs_test.split())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二次输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "docs4 = []\n",
    "for (sentiment,words) in neutral + negative  :\n",
    "    words_filtered2 = [e.lower() for e in words.split() if len(e) >= 3]  \n",
    "    docs4.append((words_filtered2, sentiment))\n",
    "#切割句子\n",
    "test_docs4 = docs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_docs4(docs4):\n",
    "    all_words = []\n",
    "    for (sentiment,words) in docs4:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)  \n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "word_features = get_word_features(get_words_in_docs4(docs4))  \n",
    "' '.join(word_features)  \n",
    "def extract_features(document):\n",
    "    document_words = set(document)  \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)   \n",
    "    return features \n",
    "#提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = nltk.classify.util.apply_features(extract_features,docs4) \n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)  \n",
    "def train(labeled_featuresets, estimator=nltk.probability.ELEProbDist):\n",
    "    # Create the P(label) distribution\n",
    "    label_probdist = estimator(label_freqdist)\n",
    "    # Create the P(fval|label, fname) distribution\n",
    "    feature_probdist = {}\n",
    "    model = NaiveBayesClassifier(label_probdist, feature_probdist)\n",
    "    return model\n",
    "#朴素贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy: 83.072334% (2228/5).\n"
     ]
    }
   ],
   "source": [
    "def classify_docs(docs5):\n",
    "    return classifier.classify(extract_features(docs5)) \n",
    "total = accuracy = float(len(test_docs4))\n",
    "for docs5 in test_docs4:\n",
    "    if classify_docs(docs5[0]) != docs5[1]:\n",
    "        accuracy -= 1\n",
    "print('Total accuracy: %f%% (%d/5).' % (accuracy / total * 100, accuracy))\n",
    "#验证效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutPut2 = []\n",
    "with open('test_x.txt', 'r') as e:\n",
    "    docs_test2 = e.read().splitlines() \n",
    "    e.close()\n",
    "    for TEST in docs_test2:\n",
    "        docs_test2 = TEST\n",
    "        OutPut2.append (classifier.classify(extract_features(docs_test2.split())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三次输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "docs6 = []\n",
    "for (sentiment,words) in neutral + positive  :\n",
    "    words_filtered3 = [e.lower() for e in words.split() if len(e) >= 3]  \n",
    "    docs6.append((words_filtered3, sentiment))\n",
    "#切割句子\n",
    "test_docs6 = docs6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_docs6(docs6):\n",
    "    all_words = []\n",
    "    for (sentiment,words) in docs6:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)  \n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "word_features = get_word_features(get_words_in_docs6(docs6))  \n",
    "' '.join(word_features)  \n",
    "def extract_features(document):\n",
    "    document_words = set(document)  \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)   \n",
    "    return features \n",
    "#提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = nltk.classify.util.apply_features(extract_features,docs6) \n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)  \n",
    "def train(labeled_featuresets, estimator=nltk.probability.ELEProbDist):\n",
    "    # Create the P(label) distribution\n",
    "    label_probdist = estimator(label_freqdist)\n",
    "    # Create the P(fval|label, fname) distribution\n",
    "    feature_probdist = {}\n",
    "    model = NaiveBayesClassifier(label_probdist, feature_probdist)\n",
    "    return model\n",
    "#朴素贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy: 67.617602% (2228/5).\n"
     ]
    }
   ],
   "source": [
    "def classify_docs(docs7):\n",
    "    return classifier.classify(extract_features(docs7)) \n",
    "total = accuracy = float(len(test_docs6))\n",
    "for docs7 in test_docs6:\n",
    "    if classify_docs(docs7[0]) != docs7[1]:\n",
    "        accuracy -= 1\n",
    "print('Total accuracy: %f%% (%d/5).' % (accuracy / total * 100, accuracy))\n",
    "#验证效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutPut3 = []\n",
    "with open('test_x.txt', 'r') as e:\n",
    "    docs_test3 = e.read().splitlines() \n",
    "    e.close()\n",
    "    for TEST in docs_test3:\n",
    "        docs_test3 = TEST\n",
    "        OutPut2.append (classifier.classify(extract_features(docs_test3.split())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
